{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOffIUgkmgJGjD3ZAFvbhwW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Scratch\n","Old code"],"metadata":{"id":"OwYh-MSXGWmL"}},{"cell_type":"markdown","source":[],"metadata":{"id":"2ZxoGxpWHKH6"}},{"cell_type":"markdown","source":["## Baseline Model\n","Training before Refactoring"],"metadata":{"id":"4g7FtxU4GatI"}},{"cell_type":"code","source":["# class ISICDataset(Dataset):\n","#     def __init__(self, df, img_dir, transform=None):\n","#         self.df = df.reset_index(drop=True)\n","#         self.img_dir = img_dir\n","#         self.transform = transform\n","#         self.labels = self.df.iloc[:, 1:].values.argmax(axis=1)\n","#         self.image_ids = self.df.iloc[:, 0].values\n","\n","#     def __len__(self):\n","#         return len(self.image_ids)\n","\n","#     def __getitem__(self, idx):\n","#         img_path = os.path.join(self.img_dir, self.image_ids[idx] + \".jpg\")\n","#         image = Image.open(img_path).convert(\"RGB\")\n","#         label = self.labels[idx]\n","#         if self.transform:\n","#             image = self.transform(image)\n","#         return image, label"],"metadata":{"id":"POUZ1ACoHHda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # OLD CODE\n","\n","# # train\n","# criterion = torch.nn.CrossEntropyLoss()\n","# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# model.train()\n","# for images, labels in train_loader:\n","#     images, labels = images.to(device), labels.to(device)\n","#     optimizer.zero_grad()\n","#     outputs = model(images)\n","#     loss = criterion(outputs, labels)\n","#     loss.backward()\n","#     optimizer.step()\n","#     print(f\"Loss: {loss.item():.4f}\")\n","\n","\n","# # eval\n","# model.eval()\n","# correct = 0\n","# total = 0\n","\n","# with torch.no_grad():\n","#     for images, labels in val_loader:\n","#         images, labels = images.to(device), labels.to(device)\n","#         outputs = model(images)\n","#         preds = torch.argmax(outputs, dim=1)\n","#         correct += (preds == labels).sum().item()\n","#         total += labels.size(0)\n","\n","# accuracy = correct / total\n","# print(f\"Validation Accuracy: {accuracy:.2%}\")"],"metadata":{"id":"FgepdR8cHo3M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Class-weighted loss\n","class_weights = compute_class_weight(\n","    class_weight=\"balanced\",\n","    classes=np.unique(train_df[\"label\"]),\n","    y=train_df[\"label\"].values\n",")\n","class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Training loop\n","num_epochs = 5\n","for epoch in range(num_epochs):\n","    print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n","    print(f\"Current device: {torch.cuda.current_device()}\")\n","    print(f\"Epoch {epoch+1} [Train]:\")\n","\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in tqdm(train_loader):\n","        images, labels = images.to(device), labels.long().to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * images.size(0)\n","\n","    avg_train_loss = running_loss / len(train_loader.dataset)\n","    print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f}\")\n","\n","    # Validation\n","    print(f\"Epoch {epoch+1} [Val]:\")\n","    model.eval()\n","    val_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(val_loader):\n","            images, labels = images.to(device), labels.long().to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item() * images.size(0)\n","\n","            preds = torch.argmax(outputs, dim=1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    val_loss /= len(val_loader.dataset)\n","    val_acc = np.mean(np.array(all_preds) == np.array(all_labels))\n","    print(f\"Epoch {epoch+1}: Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n","\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(all_labels, all_preds, target_names=[index_to_label[i] for i in range(8)]))\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(all_labels, all_preds))"],"metadata":{"id":"TnUIISKnGeYC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fusion model\n","Training loop before refactoring"],"metadata":{"id":"Qf-lpGIGGfCI"}},{"cell_type":"code","source":["# from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n","\n","# # Load pretrained EfficientNet-B3 with weights\n","# weights = EfficientNet_B3_Weights.IMAGENET1K_V1\n","# model = efficientnet_b3(weights=weights)\n","\n","# # Replace classifier head for 8-class output\n","# model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 8)\n","\n","# # Move to device\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model = model.to(device)\n","\n","# with open(\"/content/drive/My Drive/midas/models/baseline.py\", \"w\") as f:\n","#   f.write(\"# models/baseline_model.py\")\n","\n","from models.baseline_model import BaselineModel\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = BaselineModel(num_classes=8).to(device)"],"metadata":{"id":"0LC_lBfJHc_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#########\n","# Setup #\n","#########\n","\n","# use GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# prep to use class-weighted loss\n","class_weights = compute_class_weight(\n","    class_weight=\"balanced\",\n","    classes=np.unique(train_df[\"label\"]),\n","    y=train_df[\"label\"].values\n",")\n","class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n","# get metadata dim from a sample\n","_, meta_sample, _ = next(iter(train_loader))\n","meta_dim = meta_sample.shape[1]\n","# initialize model\n","model = FusionModel(num_metadata_features=meta_dim, num_classes=8).to(device)\n","# loss and optimizer\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","train_losses = []\n","val_losses = []\n","\n","#################\n","# TRAINING LOOP #\n","#################\n","\n","num_epochs = 5\n","for epoch in range(num_epochs):\n","    print(\"CUDA available:\", torch.cuda.is_available())\n","    print(\"Current device:\", torch.cuda.current_device())\n","    model.train()\n","    train_loss = 0.0\n","    try: # flag any errors that arise during trainig\n","        for imgs, metas, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n","            imgs, metas, labels = imgs.to(device), metas.to(device), labels.long().to(device)\n","            optimizer.zero_grad()\n","            outputs = model(imgs, metas)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * imgs.size(0)\n","            batch_loss = loss.item()\n","            if not np.isfinite(batch_loss):\n","                print(f\"[Error] Non-finite loss: {batch_loss} at epoch {epoch}, batch {i}\")\n","                break\n","    except Exception as e:\n","        print(f\"Exception during training: {e}\")\n","        traceback.print_exc()\n","        break\n","\n","    train_loss /= len(train_loader.dataset)\n","    train_losses.append(train_loss)\n","    print(f\"\\nEpoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for imgs, metas, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n","            imgs, metas, labels = imgs.to(device), metas.to(device), labels.long().to(device)\n","            outputs = model(imgs, metas)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item() * imgs.size(0)\n","\n","            preds = outputs.argmax(dim=1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    val_loss /= len(val_loader.dataset)\n","    val_losses.append(val_loss)\n","    val_acc = np.mean(np.array(all_preds) == np.array(all_labels))\n","    print(f\"Epoch {epoch+1}: Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n","\n","    # Per-class metrics\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(all_labels, all_preds, target_names=[index_to_label[i] for i in range(8)]))\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(all_labels, all_preds))"],"metadata":{"id":"JLRnQY5DGnD1"},"execution_count":null,"outputs":[]}]}